{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchvision"
      ],
      "metadata": {
        "id": "L2NAQSsXhwQ8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# from torchinfo import summary"
      ],
      "metadata": {
        "id": "vvppDfdaReu9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Taj8-A6id_9",
        "outputId": "cd61de35-f6b6-4259-a9c5-19c971ebeb89"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GetDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.file_list = self._get_file_list()\n",
        "\n",
        "        self.label_mapping = {}\n",
        "        label_names = os.listdir(root_dir)\n",
        "        for idx, label in enumerate(label_names):\n",
        "            self.label_mapping[label] = idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_path = self.file_list[index]\n",
        "        data = pd.read_csv(file_path).drop(['elapsed_time','frame_number','nPoint'], axis=1)\n",
        "        label = os.path.basename(os.path.dirname(file_path))\n",
        "        label_id = self.label_mapping[label]\n",
        "        # print(label)\n",
        "\n",
        "        desired_length = 100000\n",
        "        if len(data) < desired_length:\n",
        "            data = self._pad_sequence(data, desired_length)\n",
        "        else:\n",
        "            data = data[:desired_length]\n",
        "\n",
        "        return torch.tensor(data.values, dtype=torch.float32), torch.tensor(label_id, dtype=torch.long)\n",
        "\n",
        "    def _pad_sequence(self, data, desired_length):\n",
        "        data_length = len(data)\n",
        "        pad_length = desired_length - data_length\n",
        "        pad_data = pd.DataFrame([[0, 0, 0, 0, 0, 0, 0]] * pad_length, columns=data.columns)\n",
        "\n",
        "        data = pd.concat([data, pad_data], axis=0)\n",
        "        return data\n",
        "\n",
        "    def _get_file_list(self):\n",
        "        file_list = []\n",
        "        for root, dirs, files in os.walk(self.root_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(\".csv\"):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    file_list.append(file_path)\n",
        "        return file_list"
      ],
      "metadata": {
        "id": "xSVy2Jp1iipd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_root = \"/content/Train\"\n",
        "test_root = \"/content/Test\"\n",
        "\n",
        "train_dataset = GetDataset(train_root)\n",
        "test_dataset = GetDataset(test_root)"
      ],
      "metadata": {
        "id": "48WZwxadinMF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "        #self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
        "        #dropout\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        h_n = h_n[-1]\n",
        "        out = self.dropout(h_n)\n",
        "        out = self.fc2(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "m0d6fqN6iq7q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 7\n",
        "hidden_size = 128\n",
        "num_classes = 1\n",
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "learning_rate = 0.005"
      ],
      "metadata": {
        "id": "_IfKLkUpiuPb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMClassifier(input_size, hidden_size, num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "cr2 = nn.BCEWithLogitsLoss()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "tLXc0qeJiyeh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(y_hat, y):\n",
        "    return nn.BCELoss()(y_hat, y)\n",
        "def main():\n",
        "    # summary(model, (batch_size, 1, input_size))\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_data, batch_labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_data)\n",
        "            print(outputs.size(), batch_labels.size())\n",
        "            outputs = outputs.view(-1) #reshape output\n",
        "\n",
        "            loss = criterion(outputs, batch_labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
        "\n",
        "def dothis():\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            outputs = model(batch_data)\n",
        "            outputs = outputs.view(-1)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "7wU95Zl6iyww"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKOnajp8i75o",
        "outputId": "f27c9431-bfe1-4ba8-deb7-4c7b9906fbb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1]) torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dothis()"
      ],
      "metadata": {
        "id": "qhc4f4bJiiPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "# random seed.\n",
        "rand_seed = 1\n",
        "from numpy.random import seed\n",
        "seed(rand_seed)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(rand_seed)\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Flatten, Activation\n",
        "from keras.layers import Permute, Reshape\n",
        "from keras import backend as K\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import *\n",
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Flatten, Bidirectional,TimeDistributed\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "eGSOU6OqW6-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_dirs=['Walk']\n",
        "\n",
        "def one_hot_encoding(y_data, sub_dirs, categories=5):\n",
        "    Mapping=dict()\n",
        "\n",
        "    count=0\n",
        "    for i in sub_dirs:\n",
        "        Mapping[i]=count\n",
        "        count=count+1\n",
        "    print(Mapping)\n",
        "    y_features2=[]\n",
        "    print(y_data)\n",
        "    for i in y_data:\n",
        "        print(i)\n",
        "        lab=Mapping[i]\n",
        "        y_features2.append(lab)\n",
        "\n",
        "    y_features=np.array(y_features2)\n",
        "    y_features=y_features.reshape(y_features.shape[0],1)\n",
        "    from keras.utils import to_categorical\n",
        "    y_features = to_categorical(y_features)\n",
        "\n",
        "    return y_features"
      ],
      "metadata": {
        "id": "AiUwMNSlXaH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_3D_model(summary=False):\n",
        "    print('building the model ... ')\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Bidirectional(LSTM(64, return_sequences=False, stateful=False,input_shape=(60, 10*1024) )))\n",
        "    model.add(Dropout(.5,name='dropout_1'))\n",
        "    model.add(Dense(128, activation='relu', name='DENSE_1'))\n",
        "    model.add(Dropout(.5,name='dropout_2'))\n",
        "    model.add(Dense(5, activation='softmax', name = 'output'))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "uo9ocvS9XiUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_root = \"/content/Train/Walk/w_1.csv\"\n",
        "data = pd.read_csv(train_root).drop(['elapsed_time','frame_number','nPoint'], axis=1)\n",
        "train_data = np.array(data,dtype=np.dtype(np.float64))\n",
        "\n",
        "train_root = \"/content/Train/Walk/w_2.csv\"\n",
        "data = pd.read_csv(train_root).drop(['elapsed_time','frame_number','nPoint'], axis=1)\n",
        "_tr_data = np.array(data,dtype=np.dtype(np.float64))\n",
        "train_data = np.concatenate((train_data, _tr_data), axis=0)\n",
        "\n",
        "train_root = \"/content/Train/Walk/w_3.csv\"\n",
        "data = pd.read_csv(train_root).drop(['elapsed_time','frame_number','nPoint'], axis=1)\n",
        "_tr_data = np.array(data,dtype=np.dtype(np.float64))\n",
        "train_data = np.concatenate((train_data, _tr_data), axis=0)\n",
        "# train_data = np.array(train_data,dtype=np.dtype(np.int32))\n",
        "train_label = ['Walk']\n"
      ],
      "metadata": {
        "id": "RK-XpogoXvmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = np.array(train_data,dtype=np.dtype(np.int32))\n",
        "train_label = ['Walk']"
      ],
      "metadata": {
        "id": "1hxgmsJCZo5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label = one_hot_encoding(train_label, sub_dirs, categories=1)\n",
        "print(train_data.shape)\n",
        "# train_data = train_data.reshape(train_data.shape[0],train_data.shape[1], train_data.shape[2]*train_data.shape[3]*train_data.shape[4])\n"
      ],
      "metadata": {
        "id": "_ovFKYQsXwbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training Data Shape is:')\n",
        "print(train_data.shape,train_label.shape)\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val  = train_test_split(train_data, train_label, test_size=0.20, random_state=1)\n",
        "\n",
        "model = full_3D_model()\n",
        "\n",
        "\n",
        "print(\"Model building is completed\")\n",
        "\n",
        "\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None,\n",
        "                       decay=0.0, amsgrad=False)\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                   optimizer=adam,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint('/content/', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "# Training the model\n",
        "learning_hist = model.fit(X_train, y_train,\n",
        "                             batch_size=20,\n",
        "                             epochs=30,\n",
        "                             verbose=1,\n",
        "                             shuffle=True,\n",
        "                           validation_data=(X_val,y_val),\n",
        "                           callbacks=callbacks_list\n",
        "                          )"
      ],
      "metadata": {
        "id": "W3IfEr6yYcQn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}